{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification for xenophobia/hate/racism (on immigrants)\n",
    "### Datasets\n",
    "[sakren/twitter_racism_dataset](https://huggingface.co/datasets/sakren/twitter_racism_dataset) 14K rows (85% none, 15% racism)\n",
    "\n",
    "We found but did not implement these datasets:\n",
    "\n",
    "\n",
    "Hatespeech data catalogue https://hatespeechdata.com/#English-header\n",
    "\n",
    "\n",
    "SemEval-2019 Task5  https://aclanthology.org/S19-2007/\n",
    "\n",
    "\n",
    "https://github.com/aymeam/Datasets-for-Hate-Speech-Detection?tab=readme-ov-file\n",
    "\n",
    "\n",
    "## Propossed approach\n",
    "Fine-tuning a pretrained model like BERT often yields the best results. Depending on our requirements, we can choose between different versions of the model, such as BERT-large or BERT-base. Alternatively, we can train an SVM model on the same dataset, achieving relatively good results with a less complex model compared to BERT-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the HF (5-fold cross validation)\n",
    "vals_ds = load_dataset('sakren/twitter_racism_dataset', split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 100, 20)\n",
    "])\n",
    "trains_ds = load_dataset('sakren/twitter_racism_dataset', split=[\n",
    "    f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 20)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Text\"], padding='max_length', truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12124/12124 [00:00<00:00, 17603.01 examples/s]\n",
      "Map: 100%|██████████| 12124/12124 [00:00<00:00, 19083.18 examples/s]\n",
      "Map: 100%|██████████| 12123/12123 [00:00<00:00, 22238.78 examples/s]\n",
      "Map: 100%|██████████| 12124/12124 [00:00<00:00, 17508.31 examples/s]\n",
      "Map: 100%|██████████| 12124/12124 [00:00<00:00, 18365.20 examples/s]\n",
      "Map: 100%|██████████| 1347/1347 [00:00<00:00, 16936.55 examples/s]\n",
      "Map: 100%|██████████| 1347/1347 [00:00<00:00, 17364.06 examples/s]\n",
      "Map: 100%|██████████| 1348/1348 [00:00<00:00, 20794.96 examples/s]\n",
      "Map: 100%|██████████| 1347/1347 [00:00<00:00, 18450.92 examples/s]\n",
      "Map: 100%|██████████| 1347/1347 [00:00<00:00, 18617.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_encoded = [t.map(tokenize, batched=True, batch_size=None) for t in trains_ds]\n",
    "val_encoded = [t.map(tokenize, batched=True, batch_size=None) for t in vals_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encoded[0][0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting BATCH_SIZE to 64.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def order(inp):\n",
    "    '''\n",
    "    This function will group all the inputs of BERT\n",
    "    into a single dictionary and then output it with\n",
    "    labels.\n",
    "    '''\n",
    "    data = list(inp.values())\n",
    "    \n",
    "    return {\n",
    "        'input_ids': data[1],\n",
    "        'attention_mask': data[2],\n",
    "        'token_type_ids': data[3]\n",
    "    }, inp['oh_label']\n",
    "\n",
    "def convert_data(train_ds, val_ds):\n",
    "    # setting 'input_ids', 'attention_mask', 'token_type_ids', and 'label'\n",
    "    # to the tensorflow format. Now if you access this dataset you will get these\n",
    "    # columns in `tf.Tensor` format\n",
    "    train_ds.set_format('tf', \n",
    "                            columns=['input_ids', 'attention_mask', 'token_type_ids', 'oh_label'])\n",
    "\n",
    "    # converting train split of the dataset to tensorflow format\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_ds[:])\n",
    "    # set batch_size and shuffle\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
    "    # map the `order` function\n",
    "    train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # ... doing the same for test set ...\n",
    "    val_ds.set_format('tf', \n",
    "                            columns=['input_ids', 'attention_mask', 'token_type_ids', 'oh_label'])\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(val_ds[:])\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, bert_model, num_classes, dropout_rate=0.5, hidden_units=128):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.hidden_layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self.hidden_layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.bert(inputs)[1]\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'bert_model': self.bert.name,\n",
    "            'num_classes': self.num_classes,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'hidden_units': self.hidden_units\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        bert_model = TFAutoModel.from_pretrained(config.pop('bert_model'))\n",
    "        return cls(bert_model=bert_model, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 1124s 6s/step - loss: 0.3658 - accuracy: 0.8436\n",
      "22/22 [==============================] - 34s 1s/step - loss: 0.1719 - accuracy: 0.9206\n",
      "190/190 [==============================] - 1367s 7s/step - loss: 0.2518 - accuracy: 0.8983\n",
      "22/22 [==============================] - 42s 2s/step - loss: 0.1497 - accuracy: 0.9332\n",
      "190/190 [==============================] - 1029s 5s/step - loss: 0.2170 - accuracy: 0.9105\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.1583 - accuracy: 0.9280\n",
      "190/190 [==============================] - 1079s 6s/step - loss: 0.1948 - accuracy: 0.9204\n",
      "22/22 [==============================] - 36s 1s/step - loss: 0.1556 - accuracy: 0.9347\n",
      "190/190 [==============================] - 1118s 6s/step - loss: 0.1782 - accuracy: 0.9282\n",
      "22/22 [==============================] - 38s 2s/step - loss: 0.1304 - accuracy: 0.9480\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation loop\n",
    "history_list = []\n",
    "classifier = None\n",
    "for k in range(5):\n",
    "    classifier = BERTForClassification(model, num_classes=2)\n",
    "\n",
    "    classifier.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    train_dataset , test_dataset = convert_data(train_encoded[k], val_encoded[k])\n",
    "    \n",
    "    history = classifier.fit(\n",
    "        train_dataset,\n",
    "        epochs=1\n",
    "    )\n",
    "\n",
    "    classifier.evaluate(test_dataset)\n",
    "\n",
    "    history_list.append(history)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_for_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_bert_model (TFBertModel  multiple                  109482240 \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  98432     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109597442 (418.08 MB)\n",
      "Trainable params: 109597442 (418.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/bert_base_uncased_finetuned\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/bert_base_uncased_finetuned\\assets\n"
     ]
    }
   ],
   "source": [
    "# Define the input signature\n",
    "input_signature = {\n",
    "    'input_ids': tf.TensorSpec(shape=(None, 128), dtype=tf.int32, name='input_ids'),\n",
    "    'attention_mask': tf.TensorSpec(shape=(None, 128), dtype=tf.int32, name='attention_mask'),\n",
    "    'token_type_ids': tf.TensorSpec(shape=(None, 128), dtype=tf.int32, name='token_type_ids')\n",
    "}\n",
    "\n",
    "# Save the model with the input signature\n",
    "classifier._saved_model_inputs_spec = input_signature\n",
    "\n",
    "# saving the model\n",
    "classifier.save('trained_model/bert_base_uncased_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nilif\\OneDrive\\Desktop\\Task_Seraj\\seraj_env\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "loaded_model = tf.keras.models.load_model('trained_model/bert_base_uncased_finetuned', custom_objects={'BERTForClassification': BERTForClassification, 'TFBertModel': TFAutoModel.from_pretrained(\"bert-base-uncased\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9198044  0.08019559]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "input_text = \"True. Stoning and beheading were much more to Mohammed's taste.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors='tf', padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Cast the inputs to int32\n",
    "inputs = {key: tf.cast(tensor, tf.int64) for key, tensor in inputs.items()}\n",
    "\n",
    "\n",
    "# Ensure 'token_type_ids' is included in the inputs\n",
    "if 'token_type_ids' not in inputs:\n",
    "    inputs['token_type_ids'] = tf.zeros_like(inputs['input_ids'])\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model(inputs, training=False)\n",
    "\n",
    "# Print the model output\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seraj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
